#!/bin/bash

NN1="mob616"
NN2="mob508"
G1=`build_array kpi#NUM# 11 15;`
G2=`build_array kpi#NUM# 16 19;`
G3=`build_array kpi#NUM# 25 28;`
G4=`build_array kpi#NUM# 39 44;`
G5=`build_array kpi#NUM# 45 52;`
NODES="
$NN1
$NN2
$G1
$G2
$G3
$G4
$G5
"

#默认第1,2台
HIVE_NODES="
$NN1
"
#默认前5台
ZK_NODES="
kpi16
kpi25
kpi26
kpi27
kpi28
"
#以下部分影响配置文件生成
#不影响包分发
#HA中的两个namenode
NAME_NODES="
$NN1
$NN2
"
#另一个增加的namenode
STANDBY_NODE=$NN2

#所有的datanode，写进配置文件中的slaves
DATA_NODES="
$G1
$G2
$G3
$G4
$G5
"
#写进配置文件中的dfs.namenode.shared.edits.dir
#dfs.journalnode.edits.dir
#需要创建${user.home}/hadoop_journal/edits
QJOURNAL_NODES=$ZK_NODES

#resource manager
RM="$NN1"

#start-up hbase
START_HMASTER="kpi16"
#备用的hmaster
BACKUP_HMASTERS="
kpi25
kpi26
"
#regionserver 写进hbase_conf中的regionservers
RS_NODES="
kpi16
kpi25
kpi26
kpi27
kpi28
"


SSH_PORT=9922

PORT_PREFIX=57

#如果为空，将会尝试从hive-site.xml中获得,使用匹配方式获取，不保证一定获取正确
#推荐手动指定
HIVE_MYSQL="false"
#HIVE_MYSQL_HOST="xxxhost"
#HIVE_MYSQL_PORT="58306"
#HIVE_MYSQL_USER="xxxhive"
#HIVE_MYSQL_PASSWD="xxxhive"
#HIVE_MYSQL_DATABASE="xxxqiujw_hive_metastore"


#name dir用于打包成tar包备份，会将这个目录发送到HA的namenode上启动
NAME_DIR=$HOME/hadoop_temp/dfs/name


#一般都不需要改部分啊啊啊-----------

BACKUP_TAR=name-backup-`date +%Y%m%d`.tar.gz
UPGRADE_TAR=name-upgrade-`date +%Y%m%d`.tar.gz

#是否强制重新解压JAR包
FORCE_UNTAR="false"


#以下不用改啊啊啊啊----------------------------------------
NS=($NODES)
echo $HIVE_NODES
if [ -z "$HADOOP_NODES" ]; then
    HADOOP_NODES=$NODES
fi

if [ -z "$NAME_NODES" ]; then
    HAME_NODES=${NS[@]:0:2}
fi
if [ -z "$DATA_NODES" ]; then
    DATA_NODES=$NODES
fi
if [ -z "$QJOURNAL_NODES" ]; then
    QJOURNAL_NODES=$NODES
fi
if [ -z "$RM" ]; then
    RM=${NS[@]:0:1}
fi

if [ -z "$HBASE_NODES" ]; then
    HBASE_NODES=$NODES
fi
if [ -z "$BACKUP_NODES" ]; then
    BACKUP_NODES=${NS[@]:1:1}
fi
if [ -z "$RS_NODES" ]; then
    RS_NODES=${NS[@]:2:3}
fi


if [ -z "$HIVE_NODES" ]; then
    HIVE_NODES=${NS[@]:0:2}
fi

if [ -z "$ZK_NODES" ]; then
    ZK_NODES=${NS[@]:0:5}
fi


