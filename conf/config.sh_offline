#!/bin/bash

NODES="
platform30
platform31
platform32
platform33
platform34
"
#默认第1,2台
HIVE_NODES="
platform30
"
#默认前5台
ZK_NODES="
platform33
platform34
platform32
"
#以下部分影响配置文件生成
#不影响包分发
#HA中的两个namenode
NAME_NODES="
platform30
platform31
"
#另一个增加的namenode
STANDBY_NODE=platform31

#所有的datanode，写进配置文件中的slaves
DATA_NODES="
platform31
platform32
platform33
platform34
"
#写进配置文件中的dfs.namenode.shared.edits.dir
#dfs.journalnode.edits.dir
#需要创建${user.home}/hadoop_journal/edits
QJOURNAL_NODES="
platform32
platform33
platform34
"
#resource manager
RM="platform30"

START_HMASTER="platform31"
#备用的hmaster
BACKUP_HMASTERS="
platform32
"
#regionserver 写进hbase_conf中的regionservers
RS_NODES="
platform33
platform34
"


SSH_PORT=9922

PORT_PREFIX=57

#如果为空，将会尝试从hive-site.xml中获得,使用匹配方式获取，不保证一定获取正确
#推荐手动指定
HIVE_MYSQL_HOST="platform30"
HIVE_MYSQL_PORT="58306"
HIVE_MYSQL_USER="hive"
HIVE_MYSQL_PASSWD="hive"
HIVE_MYSQL_DATABASE="zhaigy1_hive_metastore"

#HIVE_MYSQL_HOST="platform33"
#HIVE_MYSQL_PORT="3306"
#HIVE_MYSQL_USER="qiujw"
#HIVE_MYSQL_PASSWD="ucbigdata"
#HIVE_MYSQL_DATABASE="hive"

#name dir用于打包成tar包备份，会将这个目录发送到HA的namenode上启动
NAME_DIR=$HOME/hadoop_temp/dfs/name


#一般都不需要改部分啊啊啊-----------

BACKUP_TAR=name-backup-`date +%Y%m%d`.tar.gz
UPGRADE_TAR=name-upgrade-`date +%Y%m%d`.tar.gz

#是否强制重新解压JAR包
FORCE_UNTAR="false"


#以下不用改啊啊啊啊----------------------------------------
NS=($NODES)
echo $HIVE_NODES
if [ -z "$HADOOP_NODES" ]; then
    HADOOP_NODES=$NODES
fi

if [ -z "$NAME_NODES" ]; then
    HAME_NODES=${NS[@]:0:2}
fi
if [ -z "$DATA_NODES" ]; then
    DATA_NODES=$NODES
fi
if [ -z "$QJOURNAL_NODES" ]; then
    QJOURNAL_NODES=$NODES
fi
if [ -z "$RM" ]; then
    RM=${NS[@]:0:1}
fi

if [ -z "$HBASE_NODES" ]; then
    HBASE_NODES=$NODES
fi
if [ -z "$BACKUP_NODES" ]; then
    BACKUP_NODES=${NS[@]:1:1}
fi
if [ -z "$RS_NODES" ]; then
    RS_NODES=${NS[@]:2:3}
fi


if [ -z "$HIVE_NODES" ]; then
    HIVE_NODES=${NS[@]:0:2}
fi

if [ -z "$ZK_NODES" ]; then
    ZK_NODES=${NS[@]:0:5}
fi


